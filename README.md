# AI-решатель для игры 2048

Этот проект представляет собой реализацию AI-агента на основе Deep Q-Networks (DQN), предназначенного для игры и попыток решения классической игры 2048. Сама игра отображается с использованием Pygame и включает плавные анимации движения и появления плиток.

## Структура проекта

-   `game.py`: Содержит основную логику игры 2048.
    -   Управляет игровым полем (сетка 4x4).
    -   Обрабатывает генерацию плиток (случайное добавление 2 или 4).
    -   Реализует механику ходов:
        -   `_compress_row`: Сдвигает плитки в одну сторону.
        -   `_merge_row`: Объединяет соседние одинаковые плитки, обновляя счет.
        -   `move_left`, `move_right`, `move_up`, `move_down`: Выполняет ходы в указанном направлении, используя трансформации доски (транспонирование, разворот).
        -   `move(direction)`: Основная функция хода, которая также вызывает добавление новой плитки, если доска изменилась.
    -   Генерирует подробные события анимации (`'move'`, `'merge'`, `'appear'`) с координатами и значениями, обеспечивая плавные переходы в UI.
    -   Проверяет состояние игры: `can_move` (есть ли возможные ходы) и `is_game_over` (игра окончена).
    -   Включает базовый интерактивный консольный режим для игры (при прямом запуске файла).

-   `ai_solver.py`: Реализует DQN (Deep Q-Network) агента с использованием PyTorch.
    -   `QNetwork(nn.Module)`: Определяет архитектуру нейронной сети (многослойный перцептрон с двумя скрытыми слоями), которая аппроксимирует Q-функцию.
    -   `DQNAgent`: Управляет процессом обучения AI.
        -   Инициализирует `policy_net` (для выбора действий) и `target_net` (для стабильных целевых Q-значений).
        -   Использует буфер воспроизведения опыта (`deque`) для хранения и выборки прошлых переходов (`состояние`, `действие`, `награда`, `следующее_состояние`, `окончено`).
        -   Реализует эпсилон-жадную стратегию выбора действий (метод `act`), балансируя между исследованием и эксплуатацией.
        -   Выполняет шаги обучения (метод `replay`): выборка мини-батчей из буфера воспроизведения, вычисление Q-целей и оптимизация `policy_net` с использованием MSE-потерь и оптимизатора Adam.
        -   Включает механизм мягкого обновления (метод `_soft_update_target_net`) для постепенного обновления весов `target_net`.
        -   Предоставляет методы для сохранения (`save`) и загрузки (`load`) весов обученной модели.
        -   Автоматически создает директорию `models_pytorch/` для сохранения моделей.

-   `main.py`: Основной скрипт для запуска игры с графическим интерфейсом на Pygame и интеграцией AI.
    -   Инициализирует окно Pygame и настраивает игровые константы (цвета, шрифты, размеры).
    -   `draw_board`: Отрисовывает игровую сетку, плитки (с числами и разными цветами в зависимости от значения) и текущий счет.
    -   Обрабатывает анимации для:
        -   Появления плиток (увеличение размера).
        -   Движения плиток (плавное перемещение из одной позиции в другую).
        -   Слияния плиток (одна плитка скользит, другая пульсирует и исчезает).
    -   Управляет списком `active_animations` для отрисовки текущих анимаций и блокировки ввода во время анимаций.
    -   `draw_game_over`: Отображает сообщение "Game Over".
    -   Основной игровой цикл:
        -   Обрабатывает ввод пользователя:
            -   Клавиши со стрелками: для ходов человека.
            -   'R': перезапуск игры.
            -   'Q': выход из игры.
            -   'M': переключение режима AI. Если модель (например, `dqn_2048_pytorch_epN.pth` из обучения) существует в директории `models_pytorch/` (используется `DEFAULT_MODEL_FILENAME` в `main.py`), AI начнет играть. Если модель не найдена, будет отображено сообщение об ошибке.
        -   Если режим AI активен и модель загружена, AI-агент выбирает и выполняет ходы.
        -   Включает небольшую задержку (`AI_MOVE_DELAY_MS`) между ходами AI для лучшего визуального наблюдения.
        -   Интегрирует `DQNAgent` из `ai_solver.py`, позволяя ему играть в игру визуально.
    -   Предоставляет функциональность для загрузки предварительно обученной модели AI.

-   `train_agent.py`: Скрипт, предназначенный для обучения DQN-агента.
    -   `get_state(board)`: Преобразует игровое поле в плоский, нормализованный вектор, подходящий для нейронной сети (логарифмическое масштабирование значений плиток по основанию 2).
    -   `calculate_reward()`: Определяет функцию вознаграждения, критически важную для направления обучения AI. Текущая функция вознаграждения поощряет:
        -   Создание новых максимальных плиток с более высоким значением (значительная награда).
        -   Общее увеличение счета.
        -   Сохранение пустых клеток.
        -   Наказывает за бесполезные ходы и окончание игры (с градуированным штрафом в зависимости от достигнутой максимальной плитки и большой наградой при достижении плитки 2048).
    -   Основной цикл обучения:
        -   Проходит через заданное количество эпизодов (`NUM_EPISODES`).
        -   В каждом эпизоде агент играет в игру максимум заданное количество шагов (`MAX_STEPS_PER_EPISODE`).
        -   Агент взаимодействует с игровой средой: выбирает действие, наблюдает следующее состояние и награду, сохраняет опыт и выполняет шаг обучения (`replay`).
        -   Отслеживает и выводит прогресс обучения (счет за эпизод, максимальная плитка, средние показатели, эпсилон).
        -   Периодически сохраняет веса модели агента.
        -   В конце обучения генерирует и сохраняет графики (`training_history_pytorch.png`), показывающие историю очков и максимальных плиток, достигнутых за эпизод.

-   `requirements.txt`: Список зависимостей проекта (Pygame, NumPy, Matplotlib, PyTorch).
-   `models_pytorch/`: Директория, в которую `train_agent.py` сохраняет файлы обученных моделей PyTorch (`.pth`), а `main.py` загружает их.
-   `README.md`: Этот файл.

## Возможности

-   Полностью играбельная игра 2048 с чистым графическим интерфейсом на Pygame.
-   Плавные и визуально привлекательные анимации для всех действий с плитками: появление, движение и слияние.
-   AI-агент на основе Deep Q-Networks (DQN), реализованный на PyTorch.
-   AI можно обучать с нуля с помощью `train_agent.py`.
-   Продуманная функция вознаграждения, разработанная для направления AI к достижению высоких результатов и формированию больших плиток.
-   Возможность сохранять обученные модели AI и загружать их, чтобы наблюдать за игрой AI в `main.py`.
-   Визуализация процесса обучения с помощью графиков очков и максимальных плиток по эпизодам.
-   Интерактивный режим для игроков-людей.
-   Переключаемый режим AI, позволяющий обученному агенту взять управление игрой на себя.
-   Настраиваемые гиперпараметры для обучения (например, скорость обучения, затухание эпсилон, размер сети, длина эпизода) в `train_agent.py`.

## Установка и запуск

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <[URL_репозитория](https://github.com/HolSoul/2048_AI_solver)>
    cd 2048_AI_solver
    ```

2.  **Создайте виртуальное окружение (рекомендуется):**
    ```bash
    python -m venv .venv
    # Windows
    .venv\Scripts\activate
    # macOS/Linux
    source .venv/bin/activate
    ```

3.  **Установите зависимости:**
    ```bash
    pip install -r requirements.txt
    ```
    *   **Примечание по PyTorch:** Файл `requirements.txt` включает `torch`, `torchvision` и `torchaudio`. Для конкретных версий CUDA или установки только для CPU может потребоваться установить PyTorch с помощью команд с [официального сайта PyTorch](https://pytorch.org/get-started/locally/).

## Как запустить

### 1. Играть в игру (человек или AI)

Чтобы запустить игру с интерфейсом Pygame:
```bash
python main.py
```
-   **Управление:**
    -   **Клавиши со стрелками:** Сделать ход (Вверх, Вниз, Влево, Вправо).
    -   **R:** Перезапустить игру.
    -   **Q:** Выйти из игры.
    -   **M:** Переключить режим AI. Если модель (например, `dqn_2048_pytorch_epN.pth` из обучения) существует в директории `models_pytorch/` (используется `DEFAULT_MODEL_FILENAME` в `main.py`), AI начнет играть. Если модель не найдена, будет отображено сообщение об ошибке.

### 2. Обучить AI-агента

Для обучения DQN-агента:
```bash
python train_agent.py
```
-   Скрипт начнет процесс обучения на основе параметров, определенных в нем (например, `NUM_EPISODES`, `MAX_STEPS_PER_EPISODE`).
-   Прогресс будет выводиться в консоль.
-   Обученные модели будут периодически сохраняться в директорию `models_pytorch/` (например, `dqn_2048_pytorch_ep100.pth`).
-   График `training_history_pytorch.png`, показывающий прогресс по очкам и максимальным плиткам, будет сохранен в корневой директории по завершении.

## Детали AI-агента

-   **Алгоритм:** Deep Q-Network (DQN).
-   **Фреймворк:** PyTorch.
-   **Архитектура нейронной сети (`QNetwork` в `ai_solver.py`):**
    -   Входной слой: Развернутое игровое поле (16 значений).
    -   Скрытый слой 1: 128 нейронов, активация ReLU.
    -   Скрытый слой 2: 128 нейронов, активация ReLU.
    -   Выходной слой: 4 нейрона (по одному для каждого возможного действия: Вверх, Вниз, Влево, Вправо), линейная активация.
-   **Представление состояния (`get_state` в `train_agent.py` и `main.py`):**
    -   Игровое поле 4x4 разворачивается в вектор из 16 элементов.
    -   Значения плиток преобразуются с помощью `log2(значение)`, чтобы сжать диапазон и подчеркнуть различия между плитками с более высокими значениями.
    -   Полученные значения нормализуются делением на `log2(2048)` (т.е. 11.0). Пустые ячейки представляются как 0.0.
-   **Функция вознаграждения (`calculate_reward` в `train_agent.py`):**
    -   Составная награда, предназначенная для поощрения эффективной игры:
        -   Положительная награда за увеличение игрового счета.
        -   Значительная положительная награда за создание новой максимальной плитки на поле с более высоким значением (масштабируется логарифмом увеличения).
        -   Небольшая положительная награда за простое наличие плиток с высоким значением.
        -   Небольшая положительная награда за увеличение количества пустых ячеек.
        -   Отрицательный штраф за ход, который не изменяет состояние доски.
        -   Значительный отрицательный штраф за окончание игры, с уменьшенными штрафами, если были достигнуты более высокие плитки (например, 64, 128). Большая положительная награда дается, если игра заканчивается достижением плитки 2048.
-   **Ключевые гиперпараметры (настраиваемые в `train_agent.py`):**
    -   `NUM_EPISODES`: Количество игр для обучения.
    -   `MAX_STEPS_PER_EPISODE`: Максимальное количество ходов, разрешенное за один игровой эпизод.
    -   `learning_rate`: Скорость обучения для оптимизатора Adam.
    -   `gamma`: Коэффициент дисконтирования для будущих наград.
    -   `epsilon_decay`: Скорость, с которой уменьшается коэффициент исследования (эпсилон).
    -   `replay_buffer_size`: Емкость памяти воспроизведения опыта.
    -   `batch_size`: Количество опытов, выбираемых из памяти для каждого шага обучения.

## Возможные будущие улучшения

-   Эксперименты с более продвинутыми RL-алгоритмами (например, Double DQN, Dueling DQN, PPO).
-   Систематическая настройка гиперпараметров (например, с использованием поиска по сетке или байесовской оптимизации).
-   Исследование различных представлений состояний или инженерии признаков.
    -   Рассмотреть возможность использования сверточных слоев, если рассматривать доску как изображение.
-   Реализовать механизм сохранения моделей с уникальными идентификаторами (например, временной меткой), чтобы предотвратить перезапись во время нескольких запусков обучения.
-   Разработать более сложный пользовательский интерфейс для обучения, выбора моделей и настройки параметров. 
